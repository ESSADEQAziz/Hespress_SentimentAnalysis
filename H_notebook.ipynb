{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h1 align=\"center\"> Analyzing Visitor Comments on Hespress</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1. Data Description and Representation**\n",
    "\n",
    "## Datasets Source\n",
    "- **Dataset Sources:**\n",
    "  - [Tweet Sentiment Multilingual Dataset on Hugging Face](https://huggingface.co/datasets/cardiffnlp/tweet_sentiment_multilingual)\n",
    "  - [DZ Sentiment YT Comments Dataset on Hugging Face](https://huggingface.co/datasets/Abdou/dz-sentiment-yt-comments)\n",
    "\n",
    "## Datasets Overview\n",
    "The project utilizes two datasets for multilingual sentiment analysis, sourced from Hugging Face:\n",
    "1. **`tweet_sentiment_multilingual`**\n",
    "2. **`dz-sentiment-yt-comments`**\n",
    "\n",
    "This dataset consists of 50,016 samples of comments extracted from Algerian YouTube channels. It is manually annotated with 3 classes (the label column) and is not balanced. Here are the number of rows of each class:\n",
    "- **`0`**: Negative 17,033 (34.06%)\n",
    "- **`1`**: Neutral 11,136 (22.26%)\n",
    "- **`2`**: Positive 21,847 (43.68%)\n",
    "\n",
    "---\n",
    "\n",
    "## File Formats\n",
    "\n",
    "### **1. JSONL Files (from `tweet_sentiment_multilingual`)**\n",
    "Each file in this dataset (`train.jsonl`, `test.jsonl`, `validate.jsonl`) is in JSON Lines format. Each line is a JSON object representing one data sample. \n",
    "\n",
    "#### Structure of JSONL File\n",
    "Each line in the JSONL file has the following key-value pairs:\n",
    "- **`text`**: A string containing the text of the tweet.\n",
    "- **`label`**: An integer (0, 1, or 2) representing the sentiment of the text.\n",
    "\n",
    "#### Example Entry in JSONL\n",
    " the text within the JSONL files is encoded in Unicode, specifically using escape sequences like \\uXXXX for non-ASCII characters. This is common when dealing with different languages, special characters, or symbols that cannot be represented directly in ASCII.\n",
    "```json\n",
    "{\"text\": \"RT @user: \\u0625\\u062d\\u0635\\u0627\\u0626\\u064a\\u0629.. \\u0627\\u0633\\u062a\\u0634\\u0647\\u0627\\u062f 96 \\ufec3\\ufed4\\ufefc\\u064b ...\", \"label\": \"0\"}\n",
    "{\"text\": \"\\u0644\\u0627 \\u0627\\u0644\\u0647 \\u0627\\u0644\\u0627 \\u0627\\u0644\\u0644\\u0647\\ud83d\\udc9c#\\u0623\\u064a\\u0641\\u0648\\u0646_\\u0627\\u0644\\u0628\\u0631\\u0648\\u0641\\u064a\\u0633\\u0648\\u0631 ...\", \"label\": \"1\"}\n",
    "\n",
    "Here, \\u0625\\u062d\\u0635\\u0627\\u0626\\u064a\\u0629 is the Unicode sequence for Ø¥Ø­ØµØ§Ø¦ÙŠØ© (Arabic word for \"statistics\").\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2. Importing Required Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import tokenizer_from_json\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import csv\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **TensorFlow (`tensorflow`)**: \n",
    "   - TensorFlow is an open-source library developed by Google for numerical computation and machine learning. It is widely used for building and training deep learning models.\n",
    "\n",
    "2. **NumPy (`numpy`)**: \n",
    "   - NumPy is a powerful library for numerical operations in Python. It is particularly useful for handling multi-dimensional arrays and performing mathematical computations efficiently.\n",
    "\n",
    "3. **NLTK (`nltk`)**:\n",
    "   - NLTK (Natural Language Toolkit) is a powerful Python library used for working with human language data (text). It provides tools and resources for a variety of tasks related to natural language processing (NLP)\n",
    "\n",
    "3. **Matplotlib (`matplotlib.pyplot`)**: \n",
    "   - Matplotlib is a plotting library for Python. It allows the creation of static, interactive, and animated visualizations, such as graphs and charts.\n",
    "\n",
    "4. **Pandas (`pandas`)**: \n",
    "   - Pandas is a data manipulation and analysis library. It provides data structures like `DataFrame` and `Series` to handle and process structured data efficiently.\n",
    "\n",
    "5. **CSV (`csv`)**: \n",
    "   - The CSV module is part of Pythonâ€™s standard library. It provides functionality for reading from and writing to CSV (Comma-Separated Values) files.\n",
    "\n",
    "6. **JSON (`json`)**: \n",
    "   - The JSON module is used for parsing and working with JSON (JavaScript Object Notation) data. JSON is a popular format for data interchange between systems.\n",
    "\n",
    "This combination of libraries sets up the environment for machine learning, data manipulation, visualization, and working with structured data formats (CSV and JSON).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3. Exploring the Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   0      1\n",
      "0                                               text  label\n",
      "1       ÙŠØ§ Ø³ÙŠ ÙƒØ±ÙŠÙ… Ø§Ù„Ø±Ø¦ÙŠØ³ Ø§Ù„Ø°Ù‰ ØªØ´ØªÙƒÙ‰ Ù„Ù‡ Ù‡Ùˆ Ø£ØµÙ„Ù‡ Ù…Ø¹ÙŠÙ†      1\n",
      "2  Ø­ØªÙ‰ Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ© Ù‚Ø§ØªÙ„ÙƒÙ… Ù…Ø§ÙƒÙ…Ø´ Ø¹Ø±Ø¨ØŒÙˆØ§Ø´ Ø¨Ù‚Ø§ ÙŠØ§ Ø¨Ø¹Ø§ØµÙŠ...      0\n",
      "3                              Thbliiii bravo souade      2\n",
      "4          ØªØ­ÙŠØ§Ù„ÙŠ Ù†Ø§Ø³ Ø¨Ù† ÙŠØ²Ù‚Ù† Ùˆ Ù„Ù‚ØµÙˆØ± ÙÙŠ Ø§Ù„ØºÙŠØ¨Ø© ğŸŒ¹ğŸ‡©ğŸ‡¿ğŸ¤£      2\n",
      "--------------------------------------------------\n",
      "            0      1\n",
      "count   50017  50017\n",
      "unique  50017      4\n",
      "top      text      2\n",
      "freq        1  21847\n",
      "--------------------------------------------------\n",
      "0    0\n",
      "1    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "data = pd.read_csv('data\\dz-sentiment-yt-comments\\ADArabic-3labels-50016.csv', sep=',', header=None)\n",
    "# Print the first 5 rows of the dataframe.\n",
    "print(data.head())\n",
    "print('-'*50)\n",
    "# Print the statistics of the data\n",
    "print(data.describe())\n",
    "print('-'*50)\n",
    "# Check for missing values\n",
    "print(data.isna().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Load the Data**:\n",
    "   - The dataset is loaded using `pandas.read_csv()`:\n",
    "     - File: `'data\\dz-sentiment-yt-comments\\ADArabic-3labels-50016.csv'`\n",
    "     - Parameters:\n",
    "       - `sep=','`: Specifies that the data is comma-separated.\n",
    "       - `header=None`: Indicates that the dataset does not have a header row, so no column names are assigned automatically.\n",
    "\n",
    "2. **View the First 5 Rows**:\n",
    "   - `data.head()`: Displays the first five rows of the dataset. This provides a quick preview of the structure and content of the data.\n",
    "\n",
    "3. **Print Summary Statistics**:\n",
    "   - `data.describe()`: Generates a summary of basic statistical details:\n",
    "     - Includes count, mean, standard deviation, minimum, maximum, and quartile values for numeric columns.\n",
    "\n",
    "4. **Check for Missing Values**:\n",
    "   - `data.isna().sum()`: Identifies missing values in each column by summing up `NaN` (Not a Number) occurrences.\n",
    "\n",
    "This step is crucial for understanding the dataset's structure, its key properties, and any data quality issues, such as missing values or irregularities.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'RT @user: @user @user   ÙˆØµÙ„Ù†Ø§ Ù„Ø§Ù‚ØªØµØ§Ø¯ Ø§Ø³ÙˆØ¡ Ù…Ù† Ø³ÙˆØ±ÙŠØ§ ÙˆØ§Ù„Ø¹Ø±Ø§Ù‚ ÙˆÙ…Ù† ØºÙŠØ± Ø­Ø±Ø¨Ø§Ù†Ø¬Ø§Ø² Ø¯Ù‡ ÙˆÙ„Ø§ Ù…Ø´ Ø§Ù†Ø¬Ø§Ø² ÙŠØ§ Ù…ØªØ¹Ù„Ù…ÙŠÙ† ÙŠØ§ Ø¨ØªÙˆØ¹ Ø§Ù„Ù…Ø¯Ø§â€¦', 'label': '0'}\n",
      "{'text': 'ÙƒØ§Ù†ÙŠ ÙˆÙŠØ³ØªØŒ Ø¯Ø±ÙŠÙƒØŒ Ù†ÙŠÙƒÙŠØŒ Ø¨ÙŠÙˆÙ†Ø³ÙŠÙ‡ØŒ Ù‚Ø§Ù‚Ø§ http', 'label': '1'}\n"
     ]
    }
   ],
   "source": [
    "jsonl_data = []\n",
    "with open('data\\\\tweet_sentiment_multilingual\\\\train.jsonl', 'r', encoding='utf-8') as jsonl_file:\n",
    "    for line in jsonl_file:\n",
    "        json_line = json.loads(line)\n",
    "        jsonl_data.append({'text': json_line['text'], 'label': json_line['label']})\n",
    "\n",
    "print(jsonl_data[0])\n",
    "print(jsonl_data[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code Breakdown:\n",
    "1. **Initialize Data Storage**:\n",
    "   - `jsonl_data = []`: Creates an empty list to store the processed data.\n",
    "\n",
    "2. **Open the JSONL File**:\n",
    "   - `with open('data\\\\tweet_sentiment_multilingual\\\\train.jsonl', 'r', encoding='utf-8') as jsonl_file`:\n",
    "     - Opens the `train.jsonl` file in read mode with UTF-8 encoding.\n",
    "     - JSONL files contain JSON objects, one per line.\n",
    "\n",
    "3. **Parse Each Line**:\n",
    "   - The file is read line by line:\n",
    "     - `json.loads(line)`: Converts each line from JSON format to a Python dictionary.\n",
    "     - Extracted Data:\n",
    "       - `'text'`: The text of the tweet/comment.\n",
    "       - `'label'`: The sentiment label.\n",
    "     - Appends the extracted information as a dictionary to the `jsonl_data` list.\n",
    "\n",
    "4. **Preview the Data**:\n",
    "   - `print(jsonl_data[0])` and `print(jsonl_data[1])`:\n",
    "     - Prints the first two entries to verify the structure and content.\n",
    "\n",
    "#### Purpose:\n",
    "This process extracts the necessary data (`text` and `label`) from the **`tweet_sentiment_multilingual`** dataset, making it ready for further analysis or combination with the second dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **4. Data Loading and Preparation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First five sentences:\n",
      "\n",
      "['Ù†ÙˆØ§Ù„ Ø§Ù„Ø²ØºØ¨ÙŠ (Ø§Ù„Ø´Ø§Ø¨ Ø®Ø§Ù„Ø¯ Ù„ÙŠØ³ Ø¹Ø§Ù„Ù…ÙŠ) Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡ Ø£ØªÙØ±Ø¬ÙŠ Ø¹Ù„Ù‰ Ù‡Ø§ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ ÙŠØ§ Ù…Ø¨ØªØ¯Ø¦Ø© http vÃ­a @user', 'ØªÙ‚ÙˆÙ„ Ù†ÙˆØ§Ù„ Ø§Ù„Ø²ØºØ¨ÙŠ : http', 'Ù†ÙˆØ§Ù„ Ø§Ù„Ø²ØºØ¨ÙŠ Ù„Ø·ÙŠÙÙ‡ Ø§Ù„ÙÙ†Ø§Ù†Ù‡ Ø§Ù„ÙˆØ­ÙŠØ¯Ù‡ Ø§Ù„Ù„ÙŠ ÙƒÙ„ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ ÙƒÙ„ÙŠØ¨Ø§Øª ØªØ¨Ø¹Ù‡Ø§ Ù…Ø§ØªØ³Ø¨Ø¨ ØªÙ„ÙˆØ« Ø¨ØµØ±ÙŠ ÙˆÙ„Ø§ Ø³Ù…Ø¹ÙŠ Ù„Ùˆ ØµÙˆØªÙ‡Ø§ Ø§Ù‚Ù„ Ù…Ù† Ø¹Ø§Ø¯ÙŠ', 'Ù„Ù…Ø§ Ù‚Ø§Ù„Øª Ù†ÙˆØ§Ù„ Ø§Ù„Ø²ØºØ¨ÙŠ Ù„Ø§Ø¨Ù‚Ù„Ù‡Ø§ Ù‡Ø§Ù„Ù„Ù‚Ø¨ ÙØ±Ø­ÙˆØ§ ÙØ§Ù†Ø²Ù‡Ø§ ğŸ˜‚ğŸ˜‚ğŸ˜‚ÙƒØ§Ù† Ù„Ø§Ø²Ù… ÙŠØ§Ø®Ø¯ÙˆÙ‡Ø§ Ø§Ù‡Ø§Ù†Ø© Ù…Ø´ Ø«Ù†Ø§Ø¡ http', 'Ø§Ù„ÙÙ†Ø§Ù†Ø© Ù†ÙˆØ§Ù„ Ø§Ù„Ø²ØºØ¨ÙŠ Ø³Ù†Ø© 90 http']\n",
      "First five labels:\n",
      "\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# method to load JSONL data\n",
    "def load_jsonl(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as jsonl_file:\n",
    "        for line in jsonl_file:\n",
    "            json_line = json.loads(line)\n",
    "            data.append({'text': json_line['text'], 'label': json_line['label']})\n",
    "    return data\n",
    "# Load JSONL data from three files\n",
    "jsonl_data1 = load_jsonl('data\\\\tweet_sentiment_multilingual\\\\test.jsonl')\n",
    "jsonl_data2 = load_jsonl('data\\\\tweet_sentiment_multilingual\\\\train.jsonl')\n",
    "jsonl_data3 = load_jsonl('data\\\\tweet_sentiment_multilingual\\\\validation.jsonl')\n",
    "# Load CSV data\n",
    "csv_data = []\n",
    "with open('data\\\\dz-sentiment-yt-comments\\\\ADArabic-3labels-50016.csv', 'r', encoding='utf-8') as csv_file:\n",
    "    reader = csv.reader(csv_file)\n",
    "    next(reader, None)  # Skip the header if there is one\n",
    "    for row in reader:\n",
    "        text = row[0]  # Assuming 'text' is in the first column\n",
    "        label = row[1]  # Assuming 'label' is in the second column\n",
    "        csv_data.append({'text': text, 'label': label})\n",
    "\n",
    "# Combine the datasets\n",
    "combined_data = jsonl_data1 + jsonl_data2 + jsonl_data2 + csv_data\n",
    "\n",
    "\n",
    "# Define the labels and sentences\n",
    "sentences = [data['text'] for data in combined_data]\n",
    "labels = [data['label'] for data in combined_data]\n",
    "# Convert labels to integers\n",
    "labels = [int(label) for label in labels]\n",
    "# Convert labels to one-hot encodings\n",
    "labels = tf.keras.utils.to_categorical(labels, num_classes=3) # converting it to numpy array instead is also an option\n",
    "# Print the first five sentences and labels\n",
    "print(f\"First five sentences:\\n\\n{sentences[:5]}\")\n",
    "print(f\"First five labels:\\n\\n{labels[:5]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method to Load JSONL Data:\n",
    "- **Function: `load_jsonl(file_path)`**\n",
    "  - This function reads a JSONL (JSON Lines) file and extracts:\n",
    "    - `'text'`: The content of the tweet/comment.\n",
    "    - `'label'`: The sentiment label.\n",
    "  - Each entry is stored as a dictionary and appended to a list.\n",
    "  - Returns a list of dictionaries containing the loaded data.\n",
    "\n",
    "#### Load Data from Multiple Sources:\n",
    "1. **Load JSONL Data**:\n",
    "   - The `load_jsonl()` function is used to read three JSONL files:\n",
    "     - `test.jsonl`\n",
    "     - `train.jsonl`\n",
    "     - `validation.jsonl`\n",
    "   - The data from these files is loaded into `jsonl_data1`, `jsonl_data2`, and `jsonl_data3`.\n",
    "\n",
    "2. **Load CSV Data**:\n",
    "   - The CSV file `ADArabic-3labels-50016.csv` is read using the `csv` module:\n",
    "     - `text`: Assumed to be in the first column.\n",
    "     - `label`: Assumed to be in the second column.\n",
    "   - The data is stored as a list of dictionaries with `text` and `label` keys.\n",
    "\n",
    "#### Combine Datasets:\n",
    "- The data from JSONL files and the CSV file is combined into a single list called `combined_data`.\n",
    "\n",
    "#### Data Transformation:\n",
    "1. **Extract Sentences and Labels**:\n",
    "   - `sentences`: A list containing all the text entries from `combined_data`.\n",
    "   - `labels`: A list containing all the sentiment labels from `combined_data`.\n",
    "\n",
    "2. **Convert Labels to Integers**:\n",
    "   - The labels are converted from strings to integers using `int()` to ensure compatibility with machine learning models.\n",
    "\n",
    "3. **One-Hot Encoding of Labels**:\n",
    "   - `tf.keras.utils.to_categorical()` is used to convert the integer labels into one-hot encoded format with three classes (positive, neutral, negative).\n",
    "\n",
    "#### Preview the Data:\n",
    "- The first five sentences and their corresponding labels are printed to verify the data structure and transformation:\n",
    "  - `sentences[:5]`: Displays the first five text entries.\n",
    "  - `labels[:5]`: Displays the first five one-hot encoded labels.\n",
    "\n",
    "#### Purpose:\n",
    "This process integrates multiple datasets from different formats (JSONL and CSV) into a unified structure and prepares the data for training machine learning models. The use of one-hot encoding ensures compatibility with models expecting categorical labels.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Punctuation\n",
    "- Use a regular expression to remove any non-word characters (except spaces) from the text.\n",
    "- This will strip punctuation marks like periods, commas, exclamation marks, etc.\n",
    "\n",
    "Example: `\"Hello, world!\"` becomes `\"Hello world\"`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove punctuation\n",
    "def remove_punctuation(text):\n",
    "    # Remove punctuation using regex\n",
    "    return re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "# Example text\n",
    "sample_text = \"Hello, world! This is an example text with punctuation.\"\n",
    "\n",
    "# Clean the text\n",
    "cleaned_text = remove_punctuation(sample_text)\n",
    "print(cleaned_text)  # Output: \"Hello world This is an example text with punctuation\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Stop Words\n",
    "- Stop words are common words in a language that are usually removed to improve model performance.\n",
    "- You can use the `nltk` library to filter out stop words in the text.\n",
    "\n",
    "Example: `\"This is an example sentence\"` becomes `\"example sentence\"`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the stop words list\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Function to remove stop words\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words('arabic'))\n",
    "    words = text.split()\n",
    "    # Remove stop words\n",
    "    return \" \".join([word for word in words if word.lower() not in stop_words])\n",
    "\n",
    "# Example text\n",
    "sample_text = \"This is an example sentence that contains stop words.\"\n",
    "\n",
    "# Clean the text\n",
    "cleaned_text = remove_stopwords(sample_text)\n",
    "print(cleaned_text)  # Output: \"example sentence contains stop words\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Duplicates\n",
    "- Duplicates in the dataset can be removed by using `drop_duplicates` in pandas, ensuring each entry is unique.\n",
    "- This step is especially useful when you have repeated text samples in the dataset.\n",
    "\n",
    "Example: \n",
    "```python\n",
    "df.drop_duplicates(subset='text')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example DataFrame\n",
    "data = {'text': [\"Hello world\", \"Hello world\", \"Python is great\", \"Python is great\", \"I love coding\"]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Remove duplicate rows\n",
    "df_unique = df.drop_duplicates(subset='text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the Data into Training and Validation Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Split the data into training and testing sets this also shuffles the data\n",
    "training_sentences, validation_sentences, training_labels, validation_labels = train_test_split(sentences, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing `train_test_split`:\n",
    "- **`train_test_split`** is a utility from the `sklearn.model_selection` module used to split datasets into training and testing (or validation) sets.\n",
    "- It also shuffles the data to ensure a random distribution of samples in each split.\n",
    "\n",
    "#### Splitting the Data:\n",
    "1. **Inputs**:\n",
    "   - `sentences`: List of all text entries (features).\n",
    "   - `labels`: Corresponding one-hot encoded labels (targets).\n",
    "\n",
    "2. **Output Variables**:\n",
    "   - `training_sentences`: Sentences used for training the model.\n",
    "   - `validation_sentences`: Sentences set aside for validation/testing.\n",
    "   - `training_labels`: Labels corresponding to the training sentences.\n",
    "   - `validation_labels`: Labels corresponding to the validation sentences.\n",
    "\n",
    "3. **Parameters**:\n",
    "   - `test_size=0.2`: Specifies that 20% of the data will be used for validation, while 80% is used for training.\n",
    "   - `random_state=42`: Ensures reproducibility by controlling the randomness of the split. The same seed (`42`) will always produce the same split.\n",
    "\n",
    "#### Purpose:\n",
    "- Splitting the dataset allows the model to be trained on one portion of the data (`training_sentences` and `training_labels`) while being evaluated on an unseen portion (`validation_sentences` and `validation_labels`).\n",
    "- This ensures the model generalizes well to new, unseen data.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing and Padding Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First sentence:\n",
      "\n",
      " Ù‡Ù„ ØªØ¹Ù„Ù… Ø§Ù† Ø§Ù„Ù†Ù‚Ø·Ù‡ (. )Ù‡ÙŠÙ‡ Ù†ÙØ³Ù‡Ø§ ğŸ‘ˆ (*)Ø¨Ø³ Ù…Ø³ÙˆÙŠÙ‡ Ø´Ø¹Ø±Ù‡Ø§ Ù…Ø«Ù„ Ù…ÙŠØ±ÙŠØ§Ù… ÙØ§Ø±Ø³ ğŸŒšğŸ˜‚âœ‹\n",
      "First sentence tokenized:\n",
      "\n",
      " [95, 1423, 18, 1, 1, 1861, 1, 407, 1, 4665, 170, 821, 488, 1]\n",
      "First sentence padded:\n",
      "\n",
      " [  95 1423   18    1    1 1861    1  407    1 4665  170  821  488    1\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0]\n",
      "padded shape:\n",
      "\n",
      " (43651, 100)\n",
      "--------------------------------------------------\n",
      "First validation sentence:\n",
      "\n",
      " Ø´ÙŠØ¦ ØªÙ‚Ø´Ø¹Ø± Ù„Ù‡ Ø§Ù„Ø£Ø¨Ø¯Ø§Ù†\n",
      "First validation sentence tokenized:\n",
      "\n",
      " [1140    1  104    1    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0]\n",
      "padded shape:\n",
      "\n",
      " (10913, 100)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "# Tokenize the sentences\n",
    "def tokenize(sentences, vocab_size, oov_token, trunc_type, padding_type, max_length):\n",
    "    tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_token)\n",
    "    tokenizer.fit_on_texts(sentences)\n",
    "    word_index = tokenizer.word_index\n",
    "    sequences = tokenizer.texts_to_sequences(sentences)\n",
    "    padded = pad_sequences(sequences, padding=padding_type, truncating=trunc_type, maxlen=max_length)\n",
    "    return sequences, padded, word_index, tokenizer\n",
    "\n",
    "vocab_size = 10000\n",
    "oov_token = \"<OOV>\"\n",
    "trunc_type = 'post'\n",
    "padding_type = 'post'\n",
    "embedding_dim = 16\n",
    "max_length = 100 # tweets don't usaually exceed 50 words\n",
    "\n",
    "train_sequences, train_padded, word_index, tokenizer = tokenize(training_sentences, vocab_size, oov_token, trunc_type, padding_type, max_length)\n",
    "\n",
    "test_padded = tokenizer.texts_to_sequences(validation_sentences)\n",
    "test_padded = pad_sequences(test_padded, padding=padding_type, truncating=trunc_type, maxlen=max_length)\n",
    "\n",
    "print(f\"First sentence:\\n\\n {training_sentences[0]}\")\n",
    "print(f\"First sentence tokenized:\\n\\n {train_sequences[0]}\")\n",
    "print(f\"First sentence padded:\\n\\n {train_padded[0]}\")\n",
    "print(f\"padded shape:\\n\\n {train_padded.shape}\")\n",
    "print(\"-\"*50)\n",
    "print(f\"First validation sentence:\\n\\n {validation_sentences[0]}\")\n",
    "print(f\"First validation sentence tokenized:\\n\\n {test_padded[0]}\")\n",
    "print(f\"padded shape:\\n\\n {test_padded.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Tokenization and Padding:\n",
    "1. **Purpose**:\n",
    "   - Neural networks cannot directly process raw text, so text needs to be converted into numerical representations. This is done using tokenization and padding.\n",
    "\n",
    "2. **Function: `tokenize()`**:\n",
    "   - **Parameters**:\n",
    "     - `sentences`: List of text data to tokenize.\n",
    "     - `vocab_size`: Maximum size of the vocabulary. The most frequent `vocab_size` words are kept.\n",
    "     - `oov_token`: Token used to replace words not found in the vocabulary (Out of Vocabulary).\n",
    "     - `trunc_type`: Specifies how sequences longer than `max_length` are truncated (`'post'` truncates at the end).\n",
    "     - `padding_type`: Specifies how sequences shorter than `max_length` are padded (`'post'` adds padding at the end).\n",
    "     - `max_length`: Maximum allowed length of sequences.\n",
    "   - **Process**:\n",
    "     - `Tokenizer()`: Initializes a tokenizer with the specified vocabulary size and OOV token.\n",
    "     - `fit_on_texts(sentences)`: Maps words in the `sentences` to unique integer indices.\n",
    "     - `texts_to_sequences(sentences)`: Converts the sentences into sequences of integers.\n",
    "     - `pad_sequences(sequences)`: Pads or truncates sequences to the specified `max_length`.\n",
    "\n",
    "3. **Outputs**:\n",
    "   - `sequences`: Tokenized sequences of integers.\n",
    "   - `padded`: Padded/truncated sequences.\n",
    "   - `word_index`: Dictionary mapping words to their token indices.\n",
    "   - `tokenizer`: Tokenizer object for later use.\n",
    "\n",
    "#### Hyperparameters:\n",
    "- `vocab_size = 10000`: Limits the vocabulary to the top 10,000 words.\n",
    "- `oov_token = \"<OOV>\"`: Assigns a special token for out-of-vocabulary words.\n",
    "- `trunc_type = 'post'` and `padding_type = 'post'`: Truncates and pads at the end of sequences.\n",
    "- `max_length = 50`: Limits the sequence length to 50 tokens (suitable for short text like tweets).\n",
    "\n",
    "#### Tokenize Training Sentences:\n",
    "- `train_sequences`: Tokenized integer sequences for the training data.\n",
    "- `train_padded`: Padded/truncated sequences for training, ensuring uniform length.\n",
    "\n",
    "#### Tokenize Validation Sentences:\n",
    "- Validation data is tokenized and padded using the same tokenizer (`tokenizer`) to ensure consistency.\n",
    "\n",
    "#### Results Preview:\n",
    "1. **First Training Sentence**:\n",
    "   - Raw text: The original sentence.\n",
    "   - Tokenized: Converted to integers using `word_index`.\n",
    "   - Padded: Adjusted to the `max_length` with padding added as necessary.\n",
    "\n",
    "2. **First Validation Sentence**:\n",
    "   - Similar steps as training data to ensure consistency.\n",
    "\n",
    "3. **Shape of Padded Data**:\n",
    "   - The shape of the padded arrays confirms uniform sequence length across the dataset.\n",
    "\n",
    "#### Purpose:\n",
    "- Tokenization and padding prepare the text data for input into machine learning models, ensuring compatibility with deep learning frameworks and maintaining sequence length uniformity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **5. Training The Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Define the model architecture\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),  # Embedding layer\n",
    "    tf.keras.layers.Dropout(0.2),  # Dropout to prevent overfitting\n",
    "    tf.keras.layers.LSTM(64, return_sequences=False),  # LSTM layer with 64 units\n",
    "    tf.keras.layers.Dense(64, activation='relu'),  # Dense layer with 64 units\n",
    "    tf.keras.layers.Dense(3, activation='softmax')  # Output layer for 3 classes\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy',  # Using categorical crossentropy for multi-class classification\n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Model for Text Classification\n",
    "\n",
    "In this section, we build and train an LSTM-based neural network for text classification. LSTM (Long Short-Term Memory) is a type of Recurrent Neural Network (RNN) designed to capture long-term dependencies in sequential data. It is particularly effective for tasks like sentiment analysis, language modeling, and time series prediction.\n",
    "\n",
    "We will use an LSTM layer to process the sequential data, followed by dense layers to output the predictions.\n",
    "\n",
    "## Model Architecture\n",
    "\n",
    "1. **Embedding Layer**: Converts words into dense vectors of fixed size.\n",
    "2. **LSTM Layer**: Captures sequential dependencies in the text.\n",
    "3. **Dense Layers**: Perform the final classification.\n",
    "4. **Output Layer**: Softmax activation for multi-class classification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation of the Model Layers\n",
    "\n",
    "1. **Embedding Layer**: \n",
    "   - The embedding layer converts the input tokens (words) into dense vectors of fixed size. This is important because it maps the sparse one-hot encoded vectors into a continuous vector space where semantically similar words are closer together.\n",
    "\n",
    "2. **Dropout Layer**: \n",
    "   - Dropout is used to reduce overfitting during training. It randomly sets a fraction of the input units to zero during training.\n",
    "\n",
    "3. **LSTM Layer**: \n",
    "   - The LSTM layer is the core component of the model. It processes the sequence data and remembers information over long sequences. The `return_sequences=False` means that we are only interested in the final output of the LSTM and not the sequence of outputs at each time step.\n",
    "\n",
    "4. **Dense Layer (64 units)**: \n",
    "   - This layer further processes the data by using a fully connected layer with 64 units and ReLU activation to learn the relationships between features.\n",
    "\n",
    "5. **Output Layer (3 units)**: \n",
    "   - The output layer has 3 units, corresponding to the 3 classes of our classification task. The `softmax` activation ensures that the model outputs probabilities for each class.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1365/1365 - 6s - loss: 0.5121 - accuracy: 0.5923 - val_loss: 0.4336 - val_accuracy: 0.6781 - 6s/epoch - 4ms/step\n",
      "Epoch 2/20\n",
      "1365/1365 - 5s - loss: 0.3900 - accuracy: 0.7165 - val_loss: 0.4003 - val_accuracy: 0.7169 - 5s/epoch - 3ms/step\n",
      "Epoch 3/20\n",
      "1365/1365 - 8s - loss: 0.3422 - accuracy: 0.7695 - val_loss: 0.3861 - val_accuracy: 0.7346 - 8s/epoch - 6ms/step\n",
      "Epoch 4/20\n",
      "1365/1365 - 7s - loss: 0.3117 - accuracy: 0.7924 - val_loss: 0.3882 - val_accuracy: 0.7333 - 7s/epoch - 5ms/step\n",
      "Epoch 5/20\n",
      "1365/1365 - 8s - loss: 0.2887 - accuracy: 0.8095 - val_loss: 0.3947 - val_accuracy: 0.7370 - 8s/epoch - 6ms/step\n",
      "Epoch 6/20\n",
      "1365/1365 - 6s - loss: 0.2698 - accuracy: 0.8231 - val_loss: 0.4135 - val_accuracy: 0.7337 - 6s/epoch - 4ms/step\n",
      "Epoch 7/20\n",
      "1365/1365 - 7s - loss: 0.2538 - accuracy: 0.8335 - val_loss: 0.4191 - val_accuracy: 0.7322 - 7s/epoch - 5ms/step\n",
      "Epoch 8/20\n",
      "1365/1365 - 9s - loss: 0.2382 - accuracy: 0.8440 - val_loss: 0.4430 - val_accuracy: 0.7286 - 9s/epoch - 7ms/step\n",
      "Epoch 9/20\n",
      "1365/1365 - 8s - loss: 0.2241 - accuracy: 0.8555 - val_loss: 0.4560 - val_accuracy: 0.7252 - 8s/epoch - 6ms/step\n",
      "Epoch 10/20\n",
      "1365/1365 - 23s - loss: 0.2111 - accuracy: 0.8633 - val_loss: 0.4735 - val_accuracy: 0.7227 - 23s/epoch - 17ms/step\n",
      "Epoch 11/20\n",
      "1365/1365 - 26s - loss: 0.1987 - accuracy: 0.8727 - val_loss: 0.5043 - val_accuracy: 0.7225 - 26s/epoch - 19ms/step\n",
      "Epoch 12/20\n",
      "1365/1365 - 29s - loss: 0.1879 - accuracy: 0.8791 - val_loss: 0.5317 - val_accuracy: 0.7273 - 29s/epoch - 22ms/step\n",
      "Epoch 13/20\n",
      "1365/1365 - 29s - loss: 0.1774 - accuracy: 0.8869 - val_loss: 0.5586 - val_accuracy: 0.7238 - 29s/epoch - 21ms/step\n",
      "Epoch 14/20\n",
      "1365/1365 - 30s - loss: 0.1677 - accuracy: 0.8921 - val_loss: 0.5779 - val_accuracy: 0.7199 - 30s/epoch - 22ms/step\n",
      "Epoch 15/20\n",
      "1365/1365 - 20s - loss: 0.1598 - accuracy: 0.8975 - val_loss: 0.6154 - val_accuracy: 0.7238 - 20s/epoch - 14ms/step\n",
      "Epoch 16/20\n",
      "1365/1365 - 20s - loss: 0.1526 - accuracy: 0.9024 - val_loss: 0.6302 - val_accuracy: 0.7200 - 20s/epoch - 15ms/step\n",
      "Epoch 17/20\n",
      "1365/1365 - 16s - loss: 0.1445 - accuracy: 0.9083 - val_loss: 0.6640 - val_accuracy: 0.7199 - 16s/epoch - 12ms/step\n",
      "Epoch 18/20\n",
      "1365/1365 - 7s - loss: 0.1385 - accuracy: 0.9113 - val_loss: 0.6923 - val_accuracy: 0.7162 - 7s/epoch - 5ms/step\n",
      "Epoch 19/20\n",
      "1365/1365 - 8s - loss: 0.1328 - accuracy: 0.9151 - val_loss: 0.7246 - val_accuracy: 0.7191 - 8s/epoch - 6ms/step\n",
      "Epoch 20/20\n",
      "1365/1365 - 6s - loss: 0.1273 - accuracy: 0.9180 - val_loss: 0.7475 - val_accuracy: 0.7214 - 6s/epoch - 5ms/step\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_padded, training_labels, epochs=20, validation_data=(test_padded, validation_labels), verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **6. Model Training Results**\n",
    "\n",
    "After training the model for 20 epochs, we can evaluate its performance on the validation set. The `fit` function will return the training and validation accuracy and loss, which can be plotted to analyze the model's performance over time.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "342/342 [==============================] - 1s 2ms/step\n",
      "Accuracy: 0.7214\n",
      "Precision: 0.7140\n",
      "Recall: 0.7214\n",
      "F1-Score: 0.7161\n"
     ]
    }
   ],
   "source": [
    "# Get the class predictions from the probabilities\n",
    "predictions = model.predict(test_padded)\n",
    "predictions = np.argmax(predictions, axis=-1)  # Convert probabilities to class labels (integer)\n",
    "\n",
    "if len(validation_labels.shape) > 1:  # Check if they are one-hot encoded\n",
    "    validation_labels = np.argmax(validation_labels, axis=-1)\n",
    "\n",
    "# Evaluate metrics\n",
    "accuracy = accuracy_score(validation_labels, predictions)\n",
    "precision = precision_score(validation_labels, predictions, average='weighted')\n",
    "recall = recall_score(validation_labels, predictions, average='weighted')\n",
    "f1 = f1_score(validation_labels, predictions, average='weighted')\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **7.Saving the model and tokenizer** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model')\n",
    "history.model.save('model.keras')\n",
    "tokenizer_json = tokenizer.to_json()\n",
    "with open('tokenizer.json', 'w', encoding='utf-8') as f:\n",
    "    f.write(tokenizer_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training the model, it's important to save both the model and tokenizer for future use (e.g., for inference or resuming training).\n",
    "\n",
    "1. **Saving the Model**:\n",
    "   - The model can be saved using the `model.save()` method in Keras. It will save the entire model architecture, weights, and training configuration.\n",
    "\n",
    "2. **Saving the Tokenizer**:\n",
    "   - The tokenizer can be saved as a JSON file using `tokenizer.to_json()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **7. Simple Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 83ms/step\n",
      "Predicted class: 2\n",
      "Predicted class label: positive\n"
     ]
    }
   ],
   "source": [
    "# Load the saved model\n",
    "model = tf.keras.models.load_model('model.keras')\n",
    "\n",
    "# Load the saved tokenizer\n",
    "with open('tokenizer.json', 'r', encoding='utf-8') as f:\n",
    "    tokenizer_json = f.read()\n",
    "    tokenizer = tokenizer_from_json(tokenizer_json)\n",
    "\n",
    "# Example Arabic sentence\n",
    "arabic_sentence = \"Ø§Ø­Ø³Ù† Ù„Ø§Ø¹Ø¨ ÙÙŠ Ø£ÙØ±ÙŠÙ‚ÙŠØ§\"  \n",
    "\n",
    "# Preprocess the sentence: Convert to sequence and pad\n",
    "sequence = tokenizer.texts_to_sequences([arabic_sentence])\n",
    "max_length = 100  # Ensure this matches the max_length used during training\n",
    "padded_sequence = tf.keras.preprocessing.sequence.pad_sequences(sequence, maxlen=max_length, padding='post')\n",
    "\n",
    "# Make predictions on the sentence\n",
    "predictions = model.predict(padded_sequence)\n",
    "\n",
    "# Get the predicted class (index of the highest probability)\n",
    "predicted_class = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Print the predicted class\n",
    "print(f\"Predicted class: {predicted_class[0]}\")  # Print the index of the predicted class\n",
    "\n",
    "# Optionally, if you have class labels, you can map them to actual labels\n",
    "class_labels = [\"neutral\", \"negative\", \"positive\"]  # Replace with your actual class labels\n",
    "print(f\"Predicted class label: {class_labels[predicted_class[0]]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
